import io
import os
import re
from urllib.parse import urlparse
from PIL import Image
from common.log import logger

def fsize(file):
    if isinstance(file, io.BytesIO):
        return file.getbuffer().nbytes
    elif isinstance(file, str):
        return os.path.getsize(file)
    elif hasattr(file, "seek") and hasattr(file, "tell"):
        pos = file.tell()
        file.seek(0, os.SEEK_END)
        size = file.tell()
        file.seek(pos)
        return size
    else:
        raise TypeError("Unsupported type")


def compress_imgfile(file, max_size):
    if fsize(file) <= max_size:
        return file
    file.seek(0)
    img = Image.open(file)
    rgb_image = img.convert("RGB")
    quality = 95
    while True:
        out_buf = io.BytesIO()
        rgb_image.save(out_buf, "JPEG", quality=quality)
        if fsize(out_buf) <= max_size:
            return out_buf
        quality -= 5


def split_string_by_utf8_length(string, max_length, max_split=0):
    encoded = string.encode("utf-8")
    start, end = 0, 0
    result = []
    while end < len(encoded):
        if max_split > 0 and len(result) >= max_split:
            result.append(encoded[start:].decode("utf-8"))
            break
        end = min(start + max_length, len(encoded))
        # å¦‚æœå½“å‰å­—èŠ‚ä¸æ˜¯ UTF-8 ç¼–ç çš„å¼€å§‹å­—èŠ‚ï¼Œåˆ™å‘å‰æŸ¥æ‰¾ç›´åˆ°æ‰¾åˆ°å¼€å§‹å­—èŠ‚ä¸ºæ­¢
        while end < len(encoded) and (encoded[end] & 0b11000000) == 0b10000000:
            end -= 1
        result.append(encoded[start:end].decode("utf-8"))
        start = end
    return result


def get_path_suffix(path):
    path = urlparse(path).path
    return os.path.splitext(path)[-1].lstrip('.')


def convert_webp_to_png(webp_image):
    from PIL import Image
    try:
        webp_image.seek(0)
        img = Image.open(webp_image).convert("RGBA")
        png_image = io.BytesIO()
        img.save(png_image, format="PNG")
        png_image.seek(0)
        return png_image
    except Exception as e:
        logger.error(f"Failed to convert WEBP to PNG: {e}")
        raise


def remove_markdown_symbol(text: str):
    # ç§»é™¤markdownæ ¼å¼ï¼Œç›®å‰å…ˆç§»é™¤**
    if not text:
        return text
    return re.sub(r'\*\*(.*?)\*\*', r'\1', text)


def parse_markdown_text(text: str):
    """
    è§£æMarkdownæ–‡æœ¬å’Œçº¯æ–‡æœ¬ä¸­çš„é“¾æ¥ï¼Œæå–æ–‡æœ¬ã€å›¾ç‰‡ã€æ–‡ä»¶ç­‰å†…å®¹
    è¿”å›æ ¼å¼: [
        {'type': 'text', 'content': 'æ–‡æœ¬å†…å®¹'},
        {'type': 'image', 'content': 'å›¾ç‰‡URL'},
        {'type': 'file', 'content': 'æ–‡ä»¶URL'},
        ...
    ]
    """
    if not text:
        return [{'type': 'text', 'content': ''}]

    logger.info(f"[PARSE] ğŸ” å¼€å§‹è§£ææ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}")
    logger.debug(f"[PARSE] è¾“å…¥æ–‡æœ¬: {repr(text[:200])}...")

    result = []
    remaining_text = text

    # å®šä¹‰æ–‡ä»¶æ‰©å±•åæ¨¡å¼ï¼ˆç”¨äºè¯†åˆ«æ–‡ä»¶é“¾æ¥ï¼‰
    file_extensions = r'\.(pdf|doc|docx|xlsx?|pptx?|txt|html?|zip|rar|7z|tar|gz|csv|json|xml)(\?[^\)]*)?$'

    # 1. æå–markdownå›¾ç‰‡ ![alt](url)
    image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
    images = re.findall(image_pattern, remaining_text)
    if images:
        logger.info(f"[PARSE] ğŸ“¸ æ‰¾åˆ° {len(images)} ä¸ªmarkdownå›¾ç‰‡")
        for alt, url in images:
            result.append({'type': 'image', 'content': url})
            logger.info(f"[PARSE] æå–markdownå›¾ç‰‡: {url}")
            # ä»æ–‡æœ¬ä¸­ç§»é™¤å·²å¤„ç†çš„å›¾ç‰‡
            remaining_text = remaining_text.replace(f'![{alt}]({url})', '', 1)

    # 2. æå–markdowné“¾æ¥ [text](url)
    link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
    links = re.findall(link_pattern, remaining_text)
    if links:
        logger.info(f"[PARSE] ğŸ”— æ‰¾åˆ° {len(links)} ä¸ªmarkdowné“¾æ¥")
        for link_text, url in links:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶é“¾æ¥
            if re.search(file_extensions, url, re.IGNORECASE):
                result.append({'type': 'file', 'content': url})
                logger.info(f"[PARSE] æå–markdownæ–‡ä»¶: {url}")
                # ä»æ–‡æœ¬ä¸­ç§»é™¤å·²å¤„ç†çš„æ–‡ä»¶é“¾æ¥
                remaining_text = remaining_text.replace(f'[{link_text}]({url})', '', 1)
            else:
                # æ™®é€šé“¾æ¥ä¿ç•™åœ¨æ–‡æœ¬ä¸­ï¼Œä½†ç§»é™¤Markdownæ ¼å¼
                remaining_text = remaining_text.replace(f'[{link_text}]({url})', link_text, 1)

    # 3. å¦‚æœæ²¡æœ‰æ‰¾åˆ°markdownæ ¼å¼çš„åª’ä½“ï¼ŒæŸ¥æ‰¾çº¯æ–‡æœ¬ä¸­çš„é“¾æ¥
    if not any(item['type'] in ['image', 'file'] for item in result):
        logger.info("[PARSE] ğŸ” æ²¡æœ‰æ‰¾åˆ°markdownåª’ä½“ï¼Œå¼€å§‹æŸ¥æ‰¾çº¯æ–‡æœ¬é“¾æ¥")

        # å›¾ç‰‡é“¾æ¥æ¨¡å¼ (é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰)
        image_url_patterns = [
            r'https://mdn\.alipayobjects\.com/[^\s]+',  # é˜¿é‡Œäº‘å›¾ç‰‡
            r'https://[^/]*\.cos\.[^/]*\.myqcloud\.com/[^\s]+\.(?:jpg|jpeg|png|gif|webp)',  # è…¾è®¯äº‘å›¾ç‰‡
            r'https://[^\s]+\.(?:jpg|jpeg|png|gif|webp|bmp)',  # é€šç”¨å›¾ç‰‡é“¾æ¥
        ]

        # æ–‡ä»¶é“¾æ¥æ¨¡å¼
        file_url_patterns = [
            r'https://[^/]*\.cos\.[^/]*\.myqcloud\.com/[^\s]+\.(?:docx?|pdf|xlsx?|pptx?)',  # è…¾è®¯äº‘æ–‡ä»¶
            r'https://[^\s]+\.(?:docx?|pdf|xlsx?|pptx?|txt|zip|rar)',  # é€šç”¨æ–‡ä»¶é“¾æ¥
        ]

        # æŸ¥æ‰¾å›¾ç‰‡é“¾æ¥
        for pattern in image_url_patterns:
            matches = re.findall(pattern, remaining_text)
            if matches:
                logger.info(f"[PARSE] ğŸ“¸ æ‰¾åˆ° {len(matches)} ä¸ªçº¯æ–‡æœ¬å›¾ç‰‡é“¾æ¥")
                for url in matches:
                    result.append({'type': 'image', 'content': url})
                    logger.info(f"[PARSE] æå–çº¯æ–‡æœ¬å›¾ç‰‡: {url}")
                    # ä»æ–‡æœ¬ä¸­ç§»é™¤é“¾æ¥
                    remaining_text = remaining_text.replace(url, '', 1)
                break  # æ‰¾åˆ°å›¾ç‰‡é“¾æ¥åå°±ä¸å†æŸ¥æ‰¾å…¶ä»–æ¨¡å¼

        # å¦‚æœæ²¡æ‰¾åˆ°å›¾ç‰‡ï¼ŒæŸ¥æ‰¾æ–‡ä»¶é“¾æ¥
        if not any(item['type'] == 'image' for item in result):
            for pattern in file_url_patterns:
                matches = re.findall(pattern, remaining_text)
                if matches:
                    logger.info(f"[PARSE] ğŸ“„ æ‰¾åˆ° {len(matches)} ä¸ªçº¯æ–‡æœ¬æ–‡ä»¶é“¾æ¥")
                    for url in matches:
                        result.append({'type': 'file', 'content': url})
                        logger.info(f"[PARSE] æå–çº¯æ–‡æœ¬æ–‡ä»¶: {url}")
                        # ä»æ–‡æœ¬ä¸­ç§»é™¤é“¾æ¥
                        remaining_text = remaining_text.replace(url, '', 1)
                    break  # æ‰¾åˆ°æ–‡ä»¶é“¾æ¥åå°±ä¸å†æŸ¥æ‰¾å…¶ä»–æ¨¡å¼

    # 4. å¤„ç†å‰©ä½™çš„æ–‡æœ¬å†…å®¹
    if remaining_text.strip():
        # ç§»é™¤å…¶ä»–Markdownæ ¼å¼
        clean_text = remaining_text
        # ç§»é™¤ç²—ä½“
        clean_text = re.sub(r'\*\*(.*?)\*\*', r'\1', clean_text)
        # ç§»é™¤æ–œä½“
        clean_text = re.sub(r'\*(.*?)\*', r'\1', clean_text)
        # ç§»é™¤ä»£ç å—
        clean_text = re.sub(r'```.*?```', '', clean_text, flags=re.DOTALL)
        # ç§»é™¤è¡Œå†…ä»£ç 
        clean_text = re.sub(r'`([^`]+)`', r'\1', clean_text)
        # æ¸…ç†å¤šä½™çš„ç©ºç™½
        clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text.strip())

        if clean_text.strip():
            result.insert(0, {'type': 'text', 'content': clean_text})
            logger.info(f"[PARSE] ğŸ“ ä¿ç•™æ–‡æœ¬å†…å®¹ï¼Œé•¿åº¦: {len(clean_text)}")

    # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œè¿”å›ç©ºæ–‡æœ¬
    if not result:
        result = [{'type': 'text', 'content': ''}]
        logger.info("[PARSE] âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å†…å®¹ï¼Œè¿”å›ç©ºæ–‡æœ¬")

    logger.info(f"[PARSE] âœ… è§£æå®Œæˆï¼Œå…± {len(result)} ä¸ªé¡¹ç›®")
    for i, item in enumerate(result):
        logger.info(f"[PARSE] é¡¹ç›® {i+1}: {item['type']} - {item['content'][:100]}...")

    return result


def print_red(text: str):
    """
    æ‰“å°çº¢è‰²æ–‡æœ¬ï¼ˆç”¨äºé”™è¯¯ä¿¡æ¯ï¼‰
    """
    # ANSIé¢œè‰²ä»£ç ï¼šçº¢è‰²
    RED = '\033[91m'
    RESET = '\033[0m'

    # åŒæ—¶è¾“å‡ºåˆ°æ—¥å¿—å’Œæ§åˆ¶å°
    logger.error(text)
    print(f"{RED}{text}{RESET}")

    return text
